%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Mobile Gesture Recognition
}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}

\author{Maxime Portaz$^{1}$% <-this % stops a space
\thanks{$^{1}$ nn }%

}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

Abstract

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Our goal is to provide a network architecture that is quickly compact and quickly usable on mobile processor, and that allows to be at the state of the art.
In the ~ \ ref {sec: smallreseaux} section, we have introduced a state of the art network of small sizes, and we rely on them to create our architecture.
Mobile Networks ~ \ cite {howard2017mobilenets}, SqueezeNet ~ \ cite {iandola2016squeezenet} or ShuffleNet ~ \ cite {zhang2017shufflenet} Simonyan and Zisserman ~ \ cite {simonyan2014y}.
The first being only use $ 3 * 3 $ convolutions, or the like.
This type of convolution is being replaced by two convolutions with fewer parameters (section ~ \ ref {sec: replace}).
The second principle to respect is two types of conversions alternating: a first does not change the size of the entries and it decreases at the same time the size of the entries, but increases the number of channels.
By linking these two types of blocks, one can build very deep networks, while retaining the longer information of the network.
We propose two new blocks S1 (section ~ \ref{sec:S1}) and S2 (section~\ref{sec:S2}).

\section{Related Work}


\section{Replacing $3*3$ Convolution}



The $ 3 * 3 $ convolutions are, since VGG ~ \ cite {simonyan2014very}, almost the only convolutions used in neural networks.
By combining several, it is possible to realize convolutions of any size, with the advantage of being able to put between each convolution a nonlinear operation (ReLU neurons) or regularization operations (\ textit {Batch-Norm} ), which improve network convergence and its generalization.
However, in the case of compact networks, convolutions account for up to 89% of the total computations made by the network ~ \ cite {ma2018shufflenet}.

To replace the $ 3 * $ 3 convolutions, we use a combination of a $ 3 * 3 $ \ textit {DeepWise} convolution, presented in the section ~ \ ref {sec: groupedConv}, and a convolution $ 1 * 1 $.
The convolution $ 3 * 3 $ \ textit {DeepWise} (DW-Conv3x3) is a convolution where each output channel is connected to only one input channel.
The convolution $ 1 * 1 $ is only interested in one pixel, on each channel.
This provides information at the neighborhood of the pixel with the first convolution, and a fusion of the information of the different channels through the second.
By combining the two as in the figure ~ \ ref {fig: conv31}, we get a close $ 3 * $ 3 convolution operation ~ \ cite {howard2017mobilenets}, with fewer parameters.



\begin{figure}%
\includegraphics[width=\columnwidth]{figures/groupConvolutionPlusFusion.png}%
\caption{$3*3$ DeepWise Convolution followed by $1*1$ convolution for fusion.}%
\label{fig:conv31}%
\end{figure}

At the complexity level, a convolution $ 3 * 3 $ on 32 input channels, with 32 output channels, contains $ (3 * 3) * 32 * 32 + 32 = 9248 $ parameters.
A convolution $ 3 * 3 $ DeepWise contains for its part only $ (3 * 3) * 32 + 32 = 320 $ parameters, as one removes all the connections between the channels, is 32 times less connection.
Convolution $ 1 * 1 $ always with 32 channels contains $ (1 * 1) * 32 * 32 + 32 = $ 1056 parameters.
The final result is a combination of the two $ 320 + 1056 = 1376 $ parameters, which represents a 85% decrease over the 9248 convolution $ 3 * 3 $.


We compare in the table ~ \ ref {tab: convG} the calculation times of the different layers on a Quad Core processor clocked at 2.60GHz, under 64-bit Linux \ footnote {All calculations presented in the following are made on the same machine }, using the Pytorch \ footnote framework {\ url {https://pytorch.org}}.
There is a decrease from 23.9 milliseconds for convolution $ 3 * 3 $ to 13.1 milliseconds for the convolution $ 3 * 3 $ DeepWise convolution and convolution $ 1 * 1 $, a decrease of 45 \%.
We use the CPU time, not the GPU, because we are interested in the deployment of our network, not its training, which is done on GPU.
To obtain stable and significant differences between the different runs, we use batches of 64 images, each with a size of $ 32 * 32 $.

\begin{table}[!htb]
\centering
\begin{tabular}{|l|c|c|}
\hline
couche & \# paramètres & temps de calcul (ms) \\
\hline
\hline
Conv3x3 & 9248 & 23.9 \\
\hline
Conv3x3G & 320 & 9.94\\
\hline
Conv1x1 & 1056 & 4.06\\
\hline
Conv3x3G + Conv1x1 & 1376 & 13.1~\footnote{A noter que le temps de calcul de la convolution $3*3$ groupée suivie de la convolution $1*1$ n'est pas égal à la somme arithmétique des temps de calcul de chacune de ces opérations prises séparément. Cela vient de l'optimisation faite par le Framework utilisé (Ici Pytorch) dans l'enchaînement des opérations.} \\
\hline
\end{tabular}
\caption{Tableau de comparaison du nombres de paramètres et du temps de calculs des convolutions groupées et non groupées}
\label{tab:convG}
\end{table}

So we have a way to replace the $ 3 * $ 3 convolutions to minimize the number of parameters, and therefore the memory occupancy, and computation times.
However, we also want to take advantage of \ textit {skip-connection} and regularization.
We present in the following two new types of blocks, replacing the convolutions $ 3 * 3 $ by what we presented, and adding connections and regularization.


\subsection {Block S1}
\label{sec:S1}


We have previously stated (section ~ \ ref {sec: cnn}) the utility of having two types of convolution blocks, with one keeping the size of the entries we call S1, and the other dividing by two the size of the inputs, by multiplying the number of channels, named S2.
Block S1 that we propose is composed of the combination of convolutions presented previously.
First a convolution $ 3 * 3 $ DeepWise, with a step of 1 and a padding of 1, to keep the size of the entries.
Then, a convolution $ 1 * 1 $ with step 1 and no filling, responsible for merging the information of the different channels.
To improve the generalization of our network, we add after each of these layers, a \ textit {batch-normalization}.
We also use replacing the convolution $ 3 * 3 $ by two convolutions to put a layer of ReLU neurons in between, which increases the non-linearity of the block.
We also propose a \ textit {skip-connection} between the entry and the exit of the block, to allow a better propagation of the gradient ~ \ cite {he2015delving}.
The figure ~ \ ref {fig: S1} shows a graphic representation of the block S1, and the code that carries out this function is indicated in appendix ~ \ ref {sec: implementationS1}.



\begin{table}[!htb]
\centering
\begin{tabular}{|c|c|}
\hline
\#couche & type de couche \\
\hline
\hline
1 & convolution $3*3$ DeepWise, pas de 1, remplissage de 1 \\
\hline
2 & \textit{batch-normalisation}\\
\hline
3 & neurones ReLU\\
\hline
4 & convolution $1*1$, pas de 1, pas de remplissage\\
\hline
5 & \textit{batch-normalisation}\\
\hline
6 & neurones ReLU \\
\hline
7 & fusion des informations entre l'entrée et la sortie par addition\\
\hline

\end{tabular}
\caption{Composition du bloc S1.}
\label{tab:S1}
\end{table}



\begin{figure}%
\centering
\includegraphics[width=.45\columnwidth]{figures/stride1.png}%
\caption{Schéma du module S1.}%
\label{fig:S1}%
\end{figure}





The computation times, reported in the table ~ \ ref {tab: blockS1}, show a large increase in computation time, due to the addition of the \ textit {batch-normalization} and ReLU layers.
If we compare to the previous table, we go from 13.1ms to 23.9ms.
However, at the parameter level, we only add those of the \ textit {batch-normalization}, which gives a total of 1504.
We compare our block with the `` Fires '' blocks of SqueezeNet ~ \ cite {iandola2016squeezenet} and `` Shuffle '' proposed by ShuffleNet ~ \ cite {zhang2017shufflenet} in the array ~ \ ref {tab: blockS1}.
These two blocks are based on the decrease in the number of channels between the two convolution layers, to reduce the number of input channels for convolution $ 3 * 3 $, which reduces the number of parameters.
Both networks must use an expensive $ 3 * $ 3 convolution in the first layer to extract the first information from the image.
Since the purpose of block S1 is to replace all convolutions, we can do without this layer, which reduces the number of parameters of the final network.
We see in the table that the S1 block is slower, 23.9ms, than the Fire block (22.3ms) and Shuffle (21.8ms).



\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
Bloc & \# paramètres & temps de calcul (ms) \\
\hline
\hline
Conv3x3DW + Conv1x1 & 1376 & 13.1 \\
\hline
Fire Bloc & 2888 & 22.3 \\
\hline
Shuffle Bloc & 472 & 21.8 \\
\hline
Bloc S1 & \textbf{1504} & \textbf{23.9}\\
\hline
\end{tabular}
\caption{Tableau de comparaison des blocs des réseaux de tailles réduites avec le bloc S1}
\label{tab:blocS1}
\end{table}



\subsection{Bloc S2}
\label{sec:S2}

To replace the convolutions that reduce the size of the inputs, including the first layer $ 3 * 3 $ used by most networks, we define the block S2
We still want to take advantage of the \ textit {skip-connection}, but this time we have a decrease in the size of the input and a change in the number of channels, so it is not possible to directly add the entrance and exit.
First of all the number of channels being different, we propose to replace the addition by a concatenation.
This makes it possible to keep the contributions of \ textit {skip-connection}, with results close to those of the addition ~ \ cite {he2015delving, zhang2017shufflenet, ma2018shufflenet}.
However, to achieve concatenation, each channel must be the same size.
We therefore propose to fix the change in size to a division by two of the size.
So, inspired by ~ \ cite {ma2018shufflenet}, applying a $ 3 * 3 $ \ textit {Average-Pooling}, with a step of 2 and a fill of 1, we get an output twice as small, It is possible to concatenate at the exit of the convolution layers.

The convolution portion of this block is very close to that of block S1, with only a change of the first layer.
The $ 3 * 3 $ DeepWise convolution layer is applied with an offset of 2 and a fill of 2, which has the effect of reducing the size of the entries by two.
The number of output channel of the block is defined as double the number of input channel.
Block S2 (figure ~ \ ref {fig: S2} represents block S2, with on the left side the Pooling operation to reduce the size of the inputs, and on the right, the layers of convolutions.


\begin{figure}%
\centering
\includegraphics[width=.6\columnwidth]{figures/stride2.png}%
\caption{Schéma du module S2.}%
\label{fig:S2}%
\end{figure}



The number of parameters, observed in the second column of the array ~ \ ref {tab: blockS2}, does not change with respect to the block S1.
We can compare it to Shuffle block with offset of 2, which uses concatenation also to merge inputs and outputs.
This block has 848 parameters, but an execution time of 14.9ms, compared to 13.2ms for the S2 block.
Block S2 is much faster than block S1, because it applies twice the convolutions $ 3 * 3 $, since it uses an offset of 1.



\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
Bloc & \# paramètres & temps de calcul (ms) \\
\hline
\hline
Shuffle Bloc Stride 2 & 848 & 14.9 \\
\hline
Bloc S2 & \textbf{1504} & \textbf{13.2}\\
\hline
\end{tabular}
\caption{Tableau de comparaison des blocs ``Shuffle Stide 2'' et S2}
\label{tab:blocS2}
\end{table}



\section{Multiframe view}

The gestures to recognize can be dynamic in the video, and to correctly identify the sequence, we want to take advantage of the information from several frames of the video.
In the previous section, we applied the neural networks on each frame of the video, classifying each of these images.
However, it is possible for a network to consider multiple images.
In the section ~ \ ref {sec: stateVideo} we presented the recursive network.
In the case of videos, recursive networks, using LSTMs or GRUs, are used on the output of a convolutional network to perform a temporal information merge.
They have the advantage of not having a limit on the number of images they can watch, but they are not suitable for mobile use, as they require that each image be passed through a CNN , which can be expensive.
By using a fixed observation window, it is possible to define a convolutional network that is interested in several images of the video, by merging the information ~ \ cite {karpathy2014large}.
In this section, we are interested in expanding our network to capture information from multiple frames, always limiting the number of parameters.

We propose new blocks that respect the information coming from different frames (section ~ \ ref {sec: FWconv}).
To merge the information of several frames, we use a convolution $ 1 * 1 $, and we evaluate the impact of the location of this fusion, at the performance level, but also at the number of parameters (section ~ \ ref {sec: melting}).



\subsection{FrameWise Convolution}
\label{sec:FWconv}

In the previous sections, we are interested only in a picture, the entrance to a network then a matrix of size $ H * W * C $, where $ M $ and $ L $ ($ 224 and $ 224 in our experiments) are the height and the width of the frames, $ C $ the number of channels (3 for RGB).
We are now interested in a set of frames, so a number of images after each other.
We can see an image sequence as a higher-dimensional matrix $ H * L * C * T $, where $ T $ is a frame number, which can also be seen as a 3-dimensional matrix: $ H * L * (C * T) $, concatenating the channels of each of the images.

Our previous S1 and S2 blocks use DeepWise convolutions, where each output channel is connected to only one input channel.
As each convolution is only interested in a single channel, increasing the number of input channels by a factor of $ T $ and the number of output channels of the same factor $ T $, the DeepWise convolution is still applicable , without mixing the information coming from several frames.
Indeed, the first $ C $ channels correspond to the image 1, the channels $] C; 2 * C] $ contain information from the second frames, and so on.

The merging of the information between the channels is done in the blocks S1 and S2 by the convolution $ 1 * 1 $.
So that it does not mix the information coming from several frames, we propose to replace it by a convolution $ 1 * 1 $ that we name \ textbf {FrameWise}.
This layer must merge the information of the channels for frames, but not the information coming from different frames.
For that, we apply the convolution by group, with a number of group equal to $ T $ the number of frames.



\section{Experimentation}



\end{document}
